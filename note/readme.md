# MySemEval2018_Task3 Experimental Logs

## Dec. 11th 2017
### 计算1
首先，我发现之前写的抽特征的函数 nltk_unigram_with_rf, nltk_unigram 调用的是 unigram，不是 nltk_unigram。因此会有一定问题，修改后我重新生成了 nltk_unigram 的词典，并计算了 rf。训练结果如下:

```
Using following features:
==============================
nltk_unigram_with_rf
hashtag_with_rf
ners_existed
wv_google
wv_GloVe
sentilexi
emoticon
punction
elongated
==============================
```

#### Train Result Table: 第一次跑 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 50.46%     | 41.04%     | 42.10%     |
| Fold 2   | 48.04%     | 40.88%     | 41.90%     |
| Fold 3   | 51.63%     | 43.82%     | 45.71%     |
| Fold 4   | 48.52%     | 44.48%     | 45.55%     |
| Fold 5   | 39.79%     | 38.94%     | 38.64%     |
| Fold 6   | 34.49%     | 36.14%     | 34.50%     |
| Fold 7   | 55.48%     | 43.29%     | 43.90%     |
| Fold 8   | 52.55%     | 39.58%     | 40.17%     |
| Fold 9   | 39.99%     | 38.87%     | 39.11%     |
| Fold 10  | 42.03%     | 38.97%     | 39.57%     |
| **Mean** | **46.30%** | **40.60%** | **41.11%** |

#### Train Result Table: 第二次跑 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 38.84%     | 39.41%     | 39.04%     |
| Fold 2   | 43.19%     | 39.85%     | 40.40%     |
| Fold 3   | 37.65%     | 37.47%     | 36.61%     |
| Fold 4   | 51.67%     | 42.74%     | 44.33%     |
| Fold 5   | 43.85%     | 39.10%     | 39.40%     |
| Fold 6   | 43.34%     | 39.49%     | 39.87%     |
| Fold 7   | 40.80%     | 37.64%     | 37.97%     |
| Fold 8   | 45.35%     | 41.76%     | 42.52%     |
| Fold 9   | 48.03%     | 43.94%     | 44.98%     |
| Fold 10  | 53.22%     | 40.96%     | 42.39%     |
| **Mean** | **44.59%** | **40.24%** | **40.75%** |

平均F1接近41%

### 计算2

添加 nltk_bigram (freq 1), 分别测试 nltk_bigram / nltk_bigram_rf。

```
Using following features:
==============================
nltk_unigram_with_rf
nltk_bigram
hashtag_with_rf
ners_existed
wv_google
wv_GloVe
sentilexi
emoticon
punction
elongated
==============================
```

#### Train Result Table: 增加了nltk_bigram_t1 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 43.56%     | 38.03%     | 37.94%     |
| Fold 2   | 49.75%     | 42.27%     | 43.01%     |
| Fold 3   | 52.88%     | 40.11%     | 41.10%     |
| Fold 4   | 47.12%     | 40.99%     | 41.20%     |
| Fold 5   | 47.18%     | 40.78%     | 41.49%     |
| Fold 6   | 54.20%     | 44.12%     | 46.32%     |
| Fold 7   | 47.49%     | 38.86%     | 39.59%     |
| Fold 8   | 50.18%     | 41.54%     | 42.37%     |
| Fold 9   | 45.15%     | 39.81%     | 40.96%     |
| Fold 10  | 53.11%     | 39.80%     | 40.95%     |
| **Mean** | **49.06%** | **40.63%** | **41.49%** |

#### Train Result Table: 增加了nltk_bigram_t1 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 40.47%     | 36.84%     | 36.74%     |
| Fold 2   | 44.97%     | 39.71%     | 39.79%     |
| Fold 3   | 47.35%     | 40.26%     | 40.63%     |
| Fold 4   | 44.60%     | 41.21%     | 40.97%     |
| Fold 5   | 42.86%     | 40.37%     | 40.63%     |
| Fold 6   | 51.47%     | 42.77%     | 43.42%     |
| Fold 7   | 40.23%     | 38.87%     | 39.06%     |
| Fold 8   | 46.03%     | 39.21%     | 39.77%     |
| Fold 9   | 50.30%     | 40.61%     | 42.01%     |
| Fold 10  | 47.36%     | 42.57%     | 43.56%     |
| **Mean** | **45.56%** | **40.24%** | **40.66%** |

#### Train Result Table: 增加了nltk_bigram_t1 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 39.20%     | 38.08%     | 37.50%     |
| Fold 2   | 40.96%     | 39.00%     | 39.16%     |
| Fold 3   | 44.41%     | 39.53%     | 40.31%     |
| Fold 4   | 61.42%     | 44.75%     | 46.12%     |
| Fold 5   | 49.87%     | 41.41%     | 42.43%     |
| Fold 6   | 50.80%     | 40.13%     | 40.90%     |
| Fold 7   | 41.75%     | 40.45%     | 40.18%     |
| Fold 8   | 44.40%     | 39.35%     | 39.88%     |
| Fold 9   | 49.97%     | 39.77%     | 40.14%     |
| Fold 10  | 44.86%     | 42.31%     | 42.64%     |
| **Mean** | **46.76%** | **40.48%** | **40.92%** |

结论：没有太大的变化。

```
Using following features:
==============================
nltk_unigram_with_rf
nltk_bigram_with_rf
hashtag_with_rf
ners_existed
wv_google
wv_GloVe
sentilexi
emoticon
punction
elongated
==============================
```

#### Train Result Table: 增加了nltk_bigram_t1_with_rf 

| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 41.01%     | 40.07%     | 39.51%     |
| Fold 2   | 66.50%     | 42.73%     | 44.15%     |
| Fold 3   | 62.16%     | 44.54%     | 46.26%     |
| Fold 4   | 42.14%     | 39.59%     | 39.95%     |
| Fold 5   | 53.15%     | 40.20%     | 40.66%     |
| Fold 6   | 41.17%     | 40.33%     | 38.83%     |
| Fold 7   | 39.91%     | 37.47%     | 37.14%     |
| Fold 8   | 52.41%     | 41.80%     | 42.94%     |
| Fold 9   | 58.93%     | 42.64%     | 44.58%     |
| Fold 10  | 52.85%     | 41.62%     | 42.91%     |
| **Mean** | **51.02%** | **41.10%** | **41.69%** |

#### Train Result Table: 增加了nltk_bigram_t1_with_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 69.00%     | 43.46%     | 44.49%     |
| Fold 2   | 62.48%     | 37.79%     | 38.24%     |
| Fold 3   | 48.07%     | 43.45%     | 44.34%     |
| Fold 4   | 63.30%     | 42.98%     | 45.55%     |
| Fold 5   | 44.81%     | 41.05%     | 40.57%     |
| Fold 6   | 39.40%     | 38.12%     | 37.49%     |
| Fold 7   | 49.46%     | 41.73%     | 42.67%     |
| Fold 8   | 49.11%     | 41.77%     | 42.72%     |
| Fold 9   | 40.45%     | 39.04%     | 38.69%     |
| Fold 10  | 47.71%     | 39.07%     | 39.40%     |
| **Mean** | **51.38%** | **40.85%** | **41.41%** |

#### Train Result Table: 增加了nltk_bigram_t1_with_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 39.14%     | 38.47%     | 37.64%     |
| Fold 2   | 48.29%     | 41.71%     | 42.52%     |
| Fold 3   | 60.87%     | 41.11%     | 42.71%     |
| Fold 4   | 47.81%     | 41.90%     | 42.94%     |
| Fold 5   | 34.95%     | 35.56%     | 33.97%     |
| Fold 6   | 53.00%     | 42.80%     | 44.52%     |
| Fold 7   | 41.53%     | 40.69%     | 40.48%     |
| Fold 8   | 39.18%     | 38.75%     | 38.47%     |
| Fold 9   | 60.54%     | 45.70%     | 47.38%     |
| Fold 10  | 49.95%     | 41.98%     | 42.68%     |
| **Mean** | **47.53%** | **40.87%** | **41.33%** |

#### 结论

使用了 nltk_bigram_t1_with_rf后，F1平均值到达了41.33%以上。

修改的文件:

dict_loader.py

dict_creator.py


### 计算 3

添加 nltk_bigram (freq 2), 分别测试 nltk_bigram / nltk_bigram_rf。

```
Using following features:
==============================
nltk_unigram_with_rf
nltk_bigram
hashtag_with_rf
ners_existed
wv_google
wv_GloVe
sentilexi
emoticon
punction
elongated
==============================
```

#### Train Result Table: 增加了nltk_bigram_t2 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 38.69%     | 39.14%     | 38.61%     |
| Fold 2   | 50.39%     | 42.12%     | 43.41%     |
| Fold 3   | 47.99%     | 43.40%     | 44.61%     |
| Fold 4   | 38.49%     | 37.75%     | 37.61%     |
| Fold 5   | 43.21%     | 39.13%     | 39.67%     |
| Fold 6   | 43.45%     | 39.70%     | 39.68%     |
| Fold 7   | 51.18%     | 39.26%     | 40.23%     |
| Fold 8   | 42.35%     | 38.32%     | 38.26%     |
| Fold 9   | 57.42%     | 44.60%     | 46.72%     |
| Fold 10  | 46.96%     | 40.81%     | 42.03%     |
| **Mean** | **46.01%** | **40.42%** | **41.09%** |

#### Train Result Table: 增加了nltk_bigram_t2 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 71.16%     | 42.82%     | 44.67%     |
| Fold 2   | 37.32%     | 37.19%     | 36.80%     |
| Fold 3   | 45.94%     | 41.34%     | 42.12%     |
| Fold 4   | 52.75%     | 44.71%     | 45.99%     |
| Fold 5   | 48.41%     | 42.83%     | 44.10%     |
| Fold 6   | 43.15%     | 39.89%     | 40.45%     |
| Fold 7   | 49.92%     | 43.25%     | 44.42%     |
| Fold 8   | 46.46%     | 40.90%     | 41.36%     |
| Fold 9   | 47.06%     | 40.58%     | 41.54%     |
| Fold 10  | 36.83%     | 36.48%     | 36.03%     |
| **Mean** | **47.90%** | **41.00%** | **41.75%** |

#### Train Result Table: 增加了nltk_bigram_t2 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 41.52%     | 40.95%     | 40.71%     |
| Fold 2   | 48.11%     | 42.43%     | 43.54%     |
| Fold 3   | 50.59%     | 44.60%     | 45.55%     |
| Fold 4   | 44.32%     | 42.81%     | 43.05%     |
| Fold 5   | 49.42%     | 40.71%     | 41.87%     |
| Fold 6   | 44.26%     | 39.23%     | 39.92%     |
| Fold 7   | 54.89%     | 45.69%     | 47.45%     |
| Fold 8   | 41.63%     | 40.48%     | 40.47%     |
| Fold 9   | 44.27%     | 39.76%     | 40.69%     |
| Fold 10  | 42.11%     | 37.69%     | 37.89%     |
| **Mean** | **46.11%** | **41.44%** | **42.11%** |

居然到了42.11%，太神奇了！	

#### Train Result Table: 增加了nltk_bigram_t2 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 50.76%     | 41.16%     | 42.68%     |
| Fold 2   | 53.70%     | 44.18%     | 45.55%     |
| Fold 3   | 58.68%     | 41.17%     | 43.14%     |
| Fold 4   | 43.11%     | 38.80%     | 39.08%     |
| Fold 5   | 47.67%     | 42.70%     | 43.31%     |
| Fold 6   | 40.14%     | 37.65%     | 37.67%     |
| Fold 7   | 35.73%     | 35.14%     | 34.99%     |
| Fold 8   | 40.37%     | 39.16%     | 39.26%     |
| Fold 9   | 43.41%     | 39.00%     | 39.36%     |
| Fold 10  | 38.98%     | 38.29%     | 38.15%     |
| **Mean** | **45.25%** | **39.72%** | **40.32%** |

PS：感觉波动比较大

#### Train Result Table: 增加了nltk_bigram_t2 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 49.95%     | 43.17%     | 44.40%     |
| Fold 2   | 35.49%     | 37.19%     | 35.50%     |
| Fold 3   | 40.33%     | 37.95%     | 38.18%     |
| Fold 4   | 43.36%     | 42.00%     | 41.95%     |
| Fold 5   | 52.49%     | 38.65%     | 40.07%     |
| Fold 6   | 40.80%     | 38.09%     | 37.66%     |
| Fold 7   | 42.67%     | 39.64%     | 39.84%     |
| Fold 8   | 46.42%     | 41.70%     | 42.18%     |
| Fold 9   | 48.37%     | 43.45%     | 44.11%     |
| Fold 10  | 44.23%     | 43.26%     | 43.39%     |
| **Mean** | **44.41%** | **40.51%** | **40.73%** |


```
Using following features:
==============================
nltk_unigram_with_rf
nltk_bigram_with_rf
hashtag_with_rf
ners_existed
wv_google
wv_GloVe
sentilexi
emoticon
punction
elongated
==============================
```

#### Train Result Table: 增加了nltk_bigram_t2_with_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 46.05%     | 43.16%     | 43.66%     |
| Fold 2   | 44.76%     | 41.09%     | 41.70%     |
| Fold 3   | 42.40%     | 40.35%     | 40.27%     |
| Fold 4   | 54.25%     | 46.35%     | 47.36%     |
| Fold 5   | 46.90%     | 42.60%     | 43.50%     |
| Fold 6   | 53.48%     | 42.06%     | 43.60%     |
| Fold 7   | 39.37%     | 38.33%     | 38.28%     |
| Fold 8   | 44.81%     | 39.28%     | 39.82%     |
| Fold 9   | 37.47%     | 38.28%     | 37.30%     |
| Fold 10  | 47.81%     | 39.79%     | 40.81%     |
| **Mean** | **45.73%** | **41.13%** | **41.63%** |

#### Train Result Table: 增加了nltk_bigram_t2_with_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 41.71%     | 38.99%     | 39.12%     |
| Fold 2   | 43.03%     | 42.56%     | 42.51%     |
| Fold 3   | 44.69%     | 41.59%     | 42.09%     |
| Fold 4   | 43.64%     | 39.67%     | 40.13%     |
| Fold 5   | 46.81%     | 38.12%     | 38.39%     |
| Fold 6   | 49.46%     | 44.55%     | 45.83%     |
| Fold 7   | 48.18%     | 38.46%     | 39.39%     |
| Fold 8   | 49.81%     | 41.03%     | 42.02%     |
| Fold 9   | 49.54%     | 43.19%     | 44.24%     |
| Fold 10  | 42.86%     | 40.37%     | 40.61%     |
| **Mean** | **45.97%** | **40.85%** | **41.43%** |

#### Train Result Table: 增加了nltk_bigram_t2_with_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 56.37%     | 41.89%     | 43.74%     |
| Fold 2   | 36.19%     | 37.23%     | 36.51%     |
| Fold 3   | 43.45%     | 41.89%     | 41.94%     |
| Fold 4   | 39.66%     | 38.88%     | 38.52%     |
| Fold 5   | 43.75%     | 43.38%     | 43.16%     |
| Fold 6   | 46.65%     | 38.16%     | 38.17%     |
| Fold 7   | 39.78%     | 40.23%     | 39.77%     |
| Fold 8   | 48.65%     | 42.02%     | 43.30%     |
| Fold 9   | 54.07%     | 43.89%     | 46.35%     |
| Fold 10  | 55.75%     | 43.71%     | 46.11%     |
| **Mean** | **46.43%** | **41.13%** | **41.76%** |

#### 结论

nltk_bigram_t2 的效果比 t1 稍微好一些。41.40%以上

有 rf 的稍微稳定一些，没有rf的不太稳定。三次rf 平均值为 41.61%

### 计算 4

添加 nltk_bigram (freq 3), 分别测试 nltk_bigram / nltk_bigram_rf。

```
Using following features:
==============================
nltk_unigram_with_rf
nltk_bigram
hashtag_with_rf
ners_existed
wv_google
wv_GloVe
sentilexi
emoticon
punction
elongated
==============================
```

#### Train Result Table: 增加了nltk_bigram_t3 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 49.39%     | 40.26%     | 41.27%     |
| Fold 2   | 46.07%     | 38.21%     | 38.72%     |
| Fold 3   | 46.63%     | 39.35%     | 40.50%     |
| Fold 4   | 52.74%     | 44.73%     | 45.89%     |
| Fold 5   | 40.45%     | 40.61%     | 40.32%     |
| Fold 6   | 48.84%     | 40.77%     | 42.58%     |
| Fold 7   | 44.22%     | 39.93%     | 40.69%     |
| Fold 8   | 48.70%     | 43.36%     | 44.33%     |
| Fold 9   | 49.06%     | 39.53%     | 40.71%     |
| Fold 10  | 41.46%     | 40.96%     | 40.96%     |
| **Mean** | **46.76%** | **40.77%** | **41.60%** |

#### Train Result Table: 增加了nltk_bigram_t3 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 46.37%     | 39.10%     | 39.68%     |
| Fold 2   | 54.98%     | 48.93%     | 50.14%     |
| Fold 3   | 42.13%     | 38.21%     | 38.50%     |
| Fold 4   | 44.91%     | 40.69%     | 41.60%     |
| Fold 5   | 72.95%     | 45.30%     | 47.33%     |
| Fold 6   | 53.37%     | 41.14%     | 41.95%     |
| Fold 7   | 44.55%     | 38.83%     | 39.44%     |
| Fold 8   | 45.27%     | 38.14%     | 38.35%     |
| Fold 9   | 55.82%     | 43.31%     | 43.88%     |
| Fold 10  | 41.00%     | 37.90%     | 37.83%     |
| **Mean** | **50.13%** | **41.16%** | **41.87%** |

```
Using following features:
==============================
nltk_unigram_with_rf
nltk_bigram_with_rf
hashtag_with_rf
ners_existed
wv_google
wv_GloVe
sentilexi
emoticon
punction
elongated
==============================
```

#### Train Result Table: 增加了nltk_bigram_t3_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 44.85%     | 40.34%     | 40.95%     |
| Fold 2   | 47.38%     | 41.81%     | 42.48%     |
| Fold 3   | 41.28%     | 37.79%     | 37.99%     |
| Fold 4   | 46.14%     | 41.13%     | 42.01%     |
| Fold 5   | 49.90%     | 43.00%     | 44.40%     |
| Fold 6   | 55.21%     | 42.26%     | 44.02%     |
| Fold 7   | 41.65%     | 40.94%     | 40.94%     |
| Fold 8   | 47.60%     | 43.53%     | 44.50%     |
| Fold 9   | 41.08%     | 38.58%     | 38.54%     |
| Fold 10  | 45.29%     | 40.38%     | 41.15%     |
| **Mean** | **46.04%** | **40.98%** | **41.70%** |

#### Train Result Table: 增加了nltk_bigram_t3_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 46.88%     | 39.80%     | 41.48%     |
| Fold 2   | 52.26%     | 45.11%     | 46.92%     |
| Fold 3   | 51.97%     | 42.45%     | 43.67%     |
| Fold 4   | 42.58%     | 40.51%     | 40.54%     |
| Fold 5   | 46.97%     | 42.28%     | 43.34%     |
| Fold 6   | 43.47%     | 41.55%     | 41.77%     |
| Fold 7   | 48.36%     | 40.29%     | 40.86%     |
| Fold 8   | 41.32%     | 40.63%     | 40.66%     |
| Fold 9   | 51.90%     | 39.52%     | 40.60%     |
| Fold 10  | 38.20%     | 39.40%     | 38.42%     |
| **Mean** | **46.39%** | **41.15%** | **41.83%** |

#### Train Result Table: 增加了nltk_bigram_t3_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 37.96%     | 37.25%     | 36.39%     |
| Fold 2   | 47.09%     | 40.57%     | 41.61%     |
| Fold 3   | 46.48%     | 39.90%     | 40.54%     |
| Fold 4   | 53.94%     | 44.83%     | 46.27%     |
| Fold 5   | 42.16%     | 42.24%     | 41.88%     |
| Fold 6   | 45.97%     | 39.14%     | 39.62%     |
| Fold 7   | 46.32%     | 42.95%     | 43.75%     |
| Fold 8   | 51.88%     | 42.41%     | 43.56%     |
| Fold 9   | 37.12%     | 37.31%     | 36.96%     |
| Fold 10  | 40.95%     | 39.63%     | 39.82%     |
| **Mean** | **44.99%** | **40.62%** | **41.04%** |

#### Train Result Table: 增加了nltk_bigram_t3_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 43.12%     | 38.98%     | 39.24%     |
| Fold 2   | 39.69%     | 38.91%     | 38.27%     |
| Fold 3   | 40.23%     | 39.35%     | 39.12%     |
| Fold 4   | 60.03%     | 45.42%     | 47.72%     |
| Fold 5   | 46.66%     | 42.02%     | 43.06%     |
| Fold 6   | 50.64%     | 44.53%     | 45.26%     |
| Fold 7   | 39.67%     | 39.72%     | 39.54%     |
| Fold 8   | 44.07%     | 39.72%     | 40.20%     |
| Fold 9   | 49.55%     | 42.29%     | 43.04%     |
| Fold 10  | 49.13%     | 41.12%     | 42.46%     |
| **Mean** | **46.28%** | **41.21%** | **41.79%** |

#### Train Result Table: 增加了nltk_bigram_t3_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 46.80%     | 41.89%     | 42.79%     |
| Fold 2   | 39.02%     | 38.03%     | 37.30%     |
| Fold 3   | 51.38%     | 47.05%     | 48.60%     |
| Fold 4   | 44.85%     | 39.73%     | 40.24%     |
| Fold 5   | 54.09%     | 42.33%     | 43.81%     |
| Fold 6   | 54.39%     | 44.80%     | 46.56%     |
| Fold 7   | 36.87%     | 36.36%     | 35.63%     |
| Fold 8   | 44.49%     | 39.60%     | 40.16%     |
| Fold 9   | 51.53%     | 44.29%     | 45.99%     |
| Fold 10  | 42.92%     | 40.12%     | 40.66%     |
| **Mean** | **46.63%** | **41.42%** | **42.17%** |

#### 结论

和 t2 没有很明显的差别。如果可以，之后不会使用 t1, t2 (因为容易过拟合问题。)

4次rf 平均值为 41.71%

### 计算 5

添加 nltk_bigram (freq 4), 分别测试 nltk_bigram / nltk_bigram_rf。

```
Using following features:
==============================
nltk_unigram_with_rf
nltk_bigram
hashtag_with_rf
ners_existed
wv_google
wv_GloVe
sentilexi
emoticon
punction
elongated
==============================
```

#### Train Result Table: 增加了nltk_bigram_t4 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 43.44%     | 38.52%     | 39.37%     |
| Fold 2   | 37.71%     | 37.93%     | 37.56%     |
| Fold 3   | 42.18%     | 40.07%     | 40.14%     |
| Fold 4   | 60.52%     | 48.23%     | 50.46%     |
| Fold 5   | 43.45%     | 40.80%     | 40.86%     |
| Fold 6   | 42.74%     | 40.46%     | 40.73%     |
| Fold 7   | 41.72%     | 36.17%     | 36.71%     |
| Fold 8   | 45.61%     | 40.72%     | 41.16%     |
| Fold 9   | 46.69%     | 39.11%     | 40.10%     |
| Fold 10  | 52.71%     | 46.03%     | 47.73%     |
| **Mean** | **45.68%** | **40.80%** | **41.48%** |

#### Train Result Table: 增加了nltk_bigram_t4 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 42.89%     | 41.38%     | 41.66%     |
| Fold 2   | 40.29%     | 38.59%     | 38.16%     |
| Fold 3   | 45.98%     | 42.01%     | 42.90%     |
| Fold 4   | 38.74%     | 38.72%     | 38.30%     |
| Fold 5   | 49.78%     | 39.11%     | 39.88%     |
| Fold 6   | 49.32%     | 41.05%     | 42.28%     |
| Fold 7   | 62.16%     | 44.98%     | 47.98%     |
| Fold 8   | 46.79%     | 41.73%     | 42.63%     |
| Fold 9   | 44.76%     | 39.53%     | 40.62%     |
| Fold 10  | 45.40%     | 39.88%     | 40.57%     |
| **Mean** | **46.61%** | **40.70%** | **41.50%** |

#### Train Result Table: 增加了nltk_bigram_t4 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 46.59%     | 41.70%     | 42.23%     |
| Fold 2   | 43.96%     | 38.32%     | 38.70%     |
| Fold 3   | 44.42%     | 37.58%     | 38.00%     |
| Fold 4   | 47.55%     | 40.71%     | 41.15%     |
| Fold 5   | 48.40%     | 44.10%     | 45.29%     |
| Fold 6   | 45.77%     | 40.54%     | 41.21%     |
| Fold 7   | 45.77%     | 36.80%     | 36.55%     |
| Fold 8   | 45.99%     | 41.31%     | 42.39%     |
| Fold 9   | 40.21%     | 40.26%     | 40.00%     |
| Fold 10  | 37.33%     | 35.22%     | 35.31%     |
| **Mean** | **44.60%** | **39.65%** | **40.08%** |

```
Using following features:
==============================
nltk_unigram_with_rf
nltk_bigram_with_rf
hashtag_with_rf
ners_existed
wv_google
wv_GloVe
sentilexi
emoticon
punction
elongated
==============================
```

#### Train Result Table: 增加了nltk_bigram_t4_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 43.12%     | 41.61%     | 41.82%     |
| Fold 2   | 60.35%     | 44.39%     | 46.62%     |
| Fold 3   | 37.14%     | 36.55%     | 36.33%     |
| Fold 4   | 54.18%     | 42.22%     | 44.47%     |
| Fold 5   | 53.24%     | 40.47%     | 41.18%     |
| Fold 6   | 48.84%     | 40.71%     | 41.93%     |
| Fold 7   | 39.02%     | 37.83%     | 37.88%     |
| Fold 8   | 40.54%     | 38.85%     | 38.87%     |
| Fold 9   | 40.44%     | 39.89%     | 39.58%     |
| Fold 10  | 48.16%     | 42.55%     | 43.46%     |
| **Mean** | **46.50%** | **40.51%** | **41.22%** |

#### Train Result Table: 增加了nltk_bigram_t4_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 45.19%     | 43.47%     | 43.72%     |
| Fold 2   | 44.13%     | 37.76%     | 38.28%     |
| Fold 3   | 39.98%     | 39.47%     | 39.19%     |
| Fold 4   | 38.19%     | 35.07%     | 35.24%     |
| Fold 5   | 40.23%     | 37.84%     | 38.12%     |
| Fold 6   | 50.83%     | 40.46%     | 42.10%     |
| Fold 7   | 40.33%     | 39.90%     | 39.52%     |
| Fold 8   | 51.75%     | 42.99%     | 44.59%     |
| Fold 9   | 44.65%     | 42.55%     | 42.80%     |
| Fold 10  | 50.51%     | 43.25%     | 44.89%     |
| **Mean** | **44.58%** | **40.28%** | **40.84%** |

#### Train Result Table: 增加了nltk_bigram_t4_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 47.30%     | 38.95%     | 39.22%     |
| Fold 2   | 35.40%     | 36.20%     | 35.42%     |
| Fold 3   | 47.53%     | 42.16%     | 43.19%     |
| Fold 4   | 45.60%     | 41.52%     | 42.04%     |
| Fold 5   | 52.20%     | 45.42%     | 47.19%     |
| Fold 6   | 45.19%     | 44.39%     | 44.46%     |
| Fold 7   | 41.74%     | 38.95%     | 39.44%     |
| Fold 8   | 42.73%     | 39.90%     | 40.51%     |
| Fold 9   | 38.45%     | 34.84%     | 35.04%     |
| Fold 10  | 53.24%     | 43.63%     | 44.48%     |
| **Mean** | **44.94%** | **40.60%** | **41.10%** |

结论：

t4的平均值为41%左右，没有t3效果好。

### 计算 6

添加 nltk_bigram (freq 5), 分别测试 nltk_bigram / nltk_bigram_rf。

#### Train Result Table: 增加了nltk_bigram_t5_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 36.44%     | 37.34%     | 36.81%     |
| Fold 2   | 46.90%     | 44.19%     | 44.95%     |
| Fold 3   | 44.04%     | 41.31%     | 41.74%     |
| Fold 4   | 41.91%     | 38.80%     | 39.11%     |
| Fold 5   | 36.49%     | 37.44%     | 36.28%     |
| Fold 6   | 46.92%     | 37.76%     | 38.80%     |
| Fold 7   | 36.54%     | 37.08%     | 36.38%     |
| Fold 8   | 53.30%     | 45.22%     | 46.70%     |
| Fold 9   | 40.52%     | 37.63%     | 37.45%     |
| Fold 10  | 54.61%     | 45.29%     | 47.39%     |
| **Mean** | **43.77%** | **40.21%** | **40.56%** |

#### Train Result Table: 增加了nltk_bigram_t5_rf 
| Fold     | Precision  | Recall     | F-1        |
| -------- | ---------- | ---------- | ---------- |
| Fold 1   | 44.62%     | 39.24%     | 39.80%     |
| Fold 2   | 45.56%     | 43.09%     | 43.65%     |
| Fold 3   | 37.23%     | 35.65%     | 35.52%     |
| Fold 4   | 51.52%     | 42.67%     | 43.71%     |
| Fold 5   | 50.35%     | 42.87%     | 44.03%     |
| Fold 6   | 38.48%     | 39.27%     | 38.49%     |
| Fold 7   | 44.97%     | 39.55%     | 39.82%     |
| Fold 8   | 41.63%     | 39.72%     | 39.93%     |
| Fold 9   | 44.54%     | 40.00%     | 41.02%     |
| Fold 10  | 37.89%     | 37.35%     | 37.02%     |
| **Mean** | **43.68%** | **39.94%** | **40.30%** |

#### 结论:

t5比不过t4。没有继续测试下的意义了。t3_rf是最好的。

## Dec. 6th 2017

This is the first time that I starting to make logs.

Until now, I have add these features to the SemEval2018_Task3 (abbr as S2018T3 in the future).

### Features

| Feature Name         | Description                              |
| -------------------- | ---------------------------------------- |
| nltk_unigram_with_rf | The unigram (each word) in each tweet. <br/>With a rf value weighting added. |
| hashtag_with_rf      | The hashtags (#morning, #like) int each tweet. <br/>With a rf value weighting added. |
| ners_existed         | Add ner information of each tweet.       |
| wv_google            | Word vector information of each tweet.   |
| wv_GloVe             | Word vector information of each tweet.   |
| sentilexi            | The dictionary of sentiment words in each tweet. |
| emoticon             | Each Emoticons in tweet, like :), `:).   |
| punction             | The punction of each tweet.              |
| elongated            | The elongated words, like !!!, hahaha, etc. |

### Efforts and results:

```
====================
Fold 1	: 41.68%
Fold 2	: 39.55%
Fold 3	: 35.18%
Fold 4	: 37.35%
Fold 5	: 36.81%
Fold 6	: 44.17%
Fold 7	: 41.97%
Fold 8	: 42.36%
Fold 9	: 37.36%
Fold 10	: 38.08%
====================
Mean	: 39.45%
```
The average of the F1-score is 39.45%-40%.


